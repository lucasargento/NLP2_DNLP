import os
import tempfile

import streamlit as st

from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.prompts import PromptTemplate

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import Chroma

# -----------------------------
# ConfiguraciÃ³n global / entorno
# -----------------------------
os.environ["ALLOW_CHROMA_TELEMETRY"] = "false"
os.environ["OPENAI_API_KEY"] = ""

# -----------------------------
# ConfiguraciÃ³n bÃ¡sica de la pÃ¡gina
# -----------------------------
st.set_page_config(
    page_title="ChateÃ¡ con tu CV",
    page_icon="ğŸ’¬",
    layout="centered",
)

st.title("ğŸ’¬ ChateÃ¡ con tu CV")
st.write(
    "Hacele preguntas al CV, comparÃ¡ contra Job Descriptions, analizÃ¡ los fragmentos utilizados para las respuestas :)"
)

# -----------------------------
# Helpers
# -----------------------------


def load_docs_from_pdf(path: str):
    """Carga un PDF y lo transforma en documentos LangChain."""
    loader = PyPDFLoader(path)
    docs = loader.load()
    return docs


def create_rag_components(docs, model_name: str, temperature: float, api_key: str):
    """
    Crea los componentes del pipeline RAG:
    - splitter
    - embeddings + Chroma + retriever
    - LLM
    - Prompt
    """

    # 1) Chunking
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=700,
        chunk_overlap=150,
        add_start_index=True,
    )
    docs_chunked = text_splitter.split_documents(docs)

    # 2) Embeddings + vector store
    embeddings = OpenAIEmbeddings(
        model="text-embedding-3-small",
        api_key=api_key,
    )
    vectordb = Chroma.from_documents(docs_chunked, embedding=embeddings)
    retriever = vectordb.as_retriever(search_kwargs={"k": 3})

    # 3) LLM
    llm = ChatOpenAI(
        model=model_name,
        temperature=temperature,
        api_key=api_key,
    )

    # 4) Prompt para forzar uso del contexto
    template = """
Eres un asistente que responde preguntas sobre el CV de un alumno.

Debes responder **Ãºnicamente** usando la informaciÃ³n del contexto
Si la respuesta no estÃ¡ en el contexto, responde exactamente:
"No tengo esa informaciÃ³n en el CV."

ğŸ“„ CONTEXTO:
{context}

â“ PREGUNTA:
{question}

ğŸ§  RESPUESTA clara, en espaÃ±ol y bien estructurada:
"""
    prompt = PromptTemplate(
        input_variables=["context", "question"],
        template=template,
    )

    return retriever, llm, prompt


def answer_question(question: str, retriever, llm, prompt: PromptTemplate):
    """Ejecuta un paso de RAG: retrieve â†’ formatear prompt â†’ llamar al modelo."""
    docs = retriever.invoke(question)
    context = "\n\n".join([d.page_content for d in docs])
    formatted_prompt = prompt.format(context=context, question=question)
    response = llm.invoke(formatted_prompt)
    return response.content, docs


# -----------------------------
# Sidebar: configuraciÃ³n y carga de CV
# -----------------------------
st.sidebar.header("âš™ï¸ ConfiguraciÃ³n")

# API Key (puede venir de env o desde la UI)
env_api_key = os.getenv("OPENAI_API_KEY") or ""
api_key = st.sidebar.text_input(
    "ğŸ”‘ OpenAI API Key",
    type="password",
    value=env_api_key,
    help="Tu clave de OpenAI. No se guarda en ningÃºn lado.",
)

if not api_key:
    st.sidebar.warning("IngresÃ¡ tu OpenAI API Key para continuar.")
    st.stop()

# Modelo y temperatura
model_name = st.sidebar.selectbox(
    "Modelo",
    options=["gpt-4o-mini", "gpt-4o", "gpt-3.5-turbo"],
    index=0,
)
temperature = st.sidebar.slider("Creatividad (temperature)", 0.0, 1.0, 0.2, 0.05)

st.sidebar.markdown("---")
st.sidebar.markdown("### ğŸ“„ CV del alumno")

uploaded_file = st.sidebar.file_uploader(
    "SubÃ­ tu CV en PDF",
    type=["pdf"],
    help="Si no subÃ­s nada, se usa el archivo por defecto en 'CV Lucas Argento.pdf' en la misma carpeta.",
)

DEFAULT_CV_PATH = "CV Lucas Argento.pdf"

# -----------------------------
# Carga de documentos
# -----------------------------
docs = None

if uploaded_file is not None:
    # Guardamos el PDF subido en un archivo temporal
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
        tmp.write(uploaded_file.read())
        tmp_path = tmp.name
    docs = load_docs_from_pdf(tmp_path)
else:
    # Usar el CV por defecto si existe
    if os.path.exists(DEFAULT_CV_PATH):
        docs = load_docs_from_pdf(DEFAULT_CV_PATH)
    else:
        st.error(
            "No se encontrÃ³ un CV. SubÃ­ un PDF desde la barra lateral "
            "o crea el archivo 'CV Lucas Argento.pdf' en esta carpeta."
        )

if docs is None:
    st.stop()

# -----------------------------
# Crear / actualizar componentes RAG
# -----------------------------
if (
    "retriever" not in st.session_state
    or st.session_state.get("qa_model") != model_name
    or st.session_state.get("qa_temp") != temperature
    or st.session_state.get("docs_source") != ("uploaded" if uploaded_file else "default")
    or st.session_state.get("qa_api_key") != api_key
):
    retriever, llm, prompt = create_rag_components(docs, model_name, temperature, api_key)
    st.session_state.retriever = retriever
    st.session_state.llm = llm
    st.session_state.prompt = prompt
    st.session_state.qa_model = model_name
    st.session_state.qa_temp = temperature
    st.session_state.docs_source = "uploaded" if uploaded_file else "default"
    st.session_state.qa_api_key = api_key

retriever = st.session_state.retriever
llm = st.session_state.llm
prompt = st.session_state.prompt

# -----------------------------
# Historial de chat
# -----------------------------
if "history" not in st.session_state:
    st.session_state.history = []

# Mostrar historial previo
for msg in st.session_state.history:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# -----------------------------
# BotÃ³n de demo automÃ¡tica
# -----------------------------
st.sidebar.markdown("---")
if st.sidebar.button("ğŸ”§ Correr demo automÃ¡tica"):
    demo_questions = [
        "Â¿DÃ³nde estudia el alumno?",
        "Â¿QuÃ© experiencia laboral tiene?",
        "Â¿QuÃ© tecnologÃ­as o lenguajes de programaciÃ³n utiliza?",
    ]
    for q in demo_questions:
        with st.chat_message("user"):
            st.markdown(q)

        answer, _docs_used = answer_question(q, retriever, llm, prompt)

        with st.chat_message("assistant"):
            st.markdown(answer)

        st.session_state.history.append({"role": "user", "content": q})
        st.session_state.history.append({"role": "assistant", "content": answer})

    st.stop()

# -----------------------------
# Input tipo chat
# -----------------------------
query = st.chat_input("Preguntame algo sobre el CV del alumno")

if query:
    # Mostrar mensaje del usuario
    with st.chat_message("user"):
        st.markdown(query)

    # Ejecutar RAG
    answer, docs_used = answer_question(query, retriever, llm, prompt)

    # Mostrar respuesta del asistente
    with st.chat_message("assistant"):
        st.markdown(answer)

    # Guardar en historial
    st.session_state.history.append({"role": "user", "content": query})
    st.session_state.history.append({"role": "assistant", "content": answer})

    # Mostrar contexto recuperado
    with st.expander("ğŸ” Ver fragmentos del CV usados para responder"):
        for i, d in enumerate(docs_used, start=1):
            st.markdown(f"**Fragmento {i}:**")
            st.write(d.page_content[:600] + ("..." if len(d.page_content) > 600 else ""))
            st.markdown("---")

# Mensaje inicial si todavÃ­a no hay conversaciÃ³n
if not st.session_state.history and query is None:
    st.info(
        "EscribÃ­ una pregunta en el cuadro de chat de abajo. Por ejemplo:\n\n"
        "- Â¿DÃ³nde estudia el alumno?\n"
        "- Â¿QuÃ© experiencia laboral tiene?\n"
        "- Â¿QuÃ© lenguajes de programaciÃ³n usa?"
    )
