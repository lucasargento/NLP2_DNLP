import os
import tempfile
import json
from typing import List, Dict, Optional, Tuple

# Configurar directorio temporal ANTES de importar librerÃ­as pesadas
# Esto soluciona problemas con PyTorch/sentence-transformers que necesitan TMPDIR
if not os.environ.get("TMPDIR"):
    # Intentar usar directorios temporales estÃ¡ndar
    _temp_dirs_to_try = ["/tmp", "/var/tmp", os.path.expanduser("~/tmp")]
    
    for tmp_dir in _temp_dirs_to_try:
        if os.path.exists(tmp_dir) and os.access(tmp_dir, os.W_OK):
            os.environ["TMPDIR"] = tmp_dir
            break
    else:
        # Crear directorio temporal local si no existe ninguno
        try:
            local_tmp = os.path.join(os.getcwd(), ".tmp")
            os.makedirs(local_tmp, exist_ok=True)
            if os.access(local_tmp, os.W_OK):
                os.environ["TMPDIR"] = local_tmp
            else:
                # Ãšltimo recurso: usar el directorio actual
                os.environ["TMPDIR"] = os.getcwd()
        except Exception:
            # Si todo falla, usar el directorio actual
            os.environ["TMPDIR"] = os.getcwd()

import streamlit as st

from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.prompts import PromptTemplate
from langchain_core.documents import Document

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import Chroma

# -----------------------------
# ConfiguraciÃ³n global / entorno
# -----------------------------
os.environ["ALLOW_CHROMA_TELEMETRY"] = "false"
os.environ["OPENAI_API_KEY"] = ""

# -----------------------------
# ConfiguraciÃ³n bÃ¡sica de la pÃ¡gina
# -----------------------------
st.set_page_config(
    page_title="Sistema Multi-Agente de CVs",
    page_icon="ğŸ¤–",
    layout="wide",
)

st.title("ChateÃ¡ con tu CV 2.0")
st.write(
    "Carga hasta 3 CVs de integrantes del equipo. El sistema detecta automÃ¡ticamente sobre quiÃ©n preguntas y enruta a los agentes correspondientes."
)

# -----------------------------
# Clase PersonAgent
# -----------------------------


class PersonAgent:
    """Agente RAG individual para una persona especÃ­fica."""

    def __init__(
        self,
        name: str,
        docs: List[Document],
        model_name: str,
        temperature: float,
        api_key: str,
    ):
        self.name = name
        self.docs = docs

        # 1) Chunking
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=700,
            chunk_overlap=150,
            add_start_index=True,
        )
        docs_chunked = text_splitter.split_documents(docs)

        # 2) Embeddings + vector store
        embeddings = OpenAIEmbeddings(
            model="text-embedding-3-small",
            api_key=api_key,
        )
        # Crear colecciÃ³n Ãºnica para cada agente usando el nombre
        # Limpiar el nombre para que sea vÃ¡lido como nombre de colecciÃ³n
        collection_name = f"cv_{name.lower().replace(' ', '_').replace('-', '_')}"
        self.vector_store = Chroma.from_documents(
            docs_chunked, 
            embedding=embeddings,
            collection_name=collection_name
        )
        self.retriever = self.vector_store.as_retriever(search_kwargs={"k": 3})

    def retrieve(self, question: str) -> Tuple[str, List[Document]]:
        """Retrieve contextos relevantes para una pregunta."""
        docs = self.retriever.invoke(question)
        context = "\n\n".join([d.page_content for d in docs])
        return context, docs

    def get_context(self, question: str) -> Tuple[str, List[Document]]:
        """Alias para retrieve."""
        return self.retrieve(question)


# -----------------------------
# Helpers
# -----------------------------


def load_docs_from_pdf(path: str) -> List[Document]:
    """Carga un PDF y lo transforma en documentos LangChain."""
    loader = PyPDFLoader(path)
    docs = loader.load()
    return docs


def is_comparative_query(query: str, llm: ChatOpenAI) -> bool:
    """
    Detecta si una pregunta requiere comparaciÃ³n o selecciÃ³n entre mÃºltiples personas.
    
    Args:
        query: Pregunta del usuario
        llm: Modelo LLM para detecciÃ³n
        
    Returns:
        True si la pregunta requiere comparaciÃ³n/selecciÃ³n, False en caso contrario
    """
    prompt_text = f"""Analiza la siguiente pregunta y determina si requiere comparar o seleccionar entre mÃºltiples personas/candidatos.

Pregunta: "{query}"

Responde SOLO con "true" o "false" (sin comillas, sin texto adicional).

Una pregunta es comparativa si:
- Pregunta quiÃ©n es el mejor/mÃ¡s adecuado para algo
- Pregunta quiÃ©n tiene mÃ¡s/menos de algo
- Pregunta comparar habilidades/experiencias entre personas
- Pregunta recomendar quiÃ©n para un trabajo/rol
- Pregunta diferencias entre candidatos

Ejemplos:
- "Â¿QuiÃ©n es el mejor fit para programaciÃ³n?" â†’ true
- "Â¿QuiÃ©n tiene mÃ¡s experiencia en Python?" â†’ true
- "Compara las habilidades de los candidatos" â†’ true
- "Â¿QuiÃ©n recomiendas para este trabajo?" â†’ true
- "Â¿QuÃ© experiencia tiene Juan?" â†’ false
- "Â¿DÃ³nde estudia MarÃ­a?" â†’ false
- "Â¿QuÃ© tecnologÃ­as usa?" â†’ false

Respuesta (solo true o false):"""

    try:
        response = llm.invoke(prompt_text)
        content = response.content.strip().lower()
        
        # Limpiar respuesta si tiene markdown code blocks
        if content.startswith("```"):
            lines = content.split("\n")
            if lines[-1].strip() == "```":
                content = "\n".join(lines[1:-1]).strip().lower()
            else:
                content = "\n".join(lines[1:]).strip().lower()
        
        return content == "true" or content.startswith("true")
    except Exception as e:
        # Si falla la detecciÃ³n, usar heurÃ­stica simple
        comparative_keywords = [
            "mejor", "peor", "mÃ¡s", "menos", "comparar", "comparaciÃ³n",
            "recomendar", "recomendaciÃ³n", "fit", "adecuado", "suitable",
            "diferencias", "quiÃ©n", "cual", "seleccionar", "elegir"
        ]
        query_lower = query.lower()
        return any(keyword in query_lower for keyword in comparative_keywords)


def detect_people_in_query(
    query: str, available_people: List[str], llm: ChatOpenAI
) -> List[str]:
    """
    Detecta quÃ© personas se mencionan en la query usando un LLM.
    
    Args:
        query: Pregunta del usuario
        available_people: Lista de nombres de personas disponibles
        llm: Modelo LLM para detecciÃ³n
        
    Returns:
        Lista de nombres detectados (puede estar vacÃ­a)
    """
    if not available_people:
        return []

    prompt_text = f"""Analiza la siguiente pregunta y determina si menciona alguna de estas personas: {', '.join(available_people)}

Pregunta: "{query}"

Responde SOLO con un JSON array de los nombres mencionados. Si no se menciona ninguna persona, responde con un array vacÃ­o [].

Ejemplos:
- "Â¿QuÃ© experiencia tiene Juan?" â†’ ["Juan"]
- "Â¿DÃ³nde estudia MarÃ­a?" â†’ ["MarÃ­a"]
- "Compara las habilidades de Juan y Pedro" â†’ ["Juan", "Pedro"]
- "Â¿QuÃ© tecnologÃ­as usa?" â†’ []

Respuesta (solo JSON, sin texto adicional):"""

    try:
        response = llm.invoke(prompt_text)
        content = response.content.strip()

        # Limpiar respuesta si tiene markdown code blocks
        if content.startswith("```"):
            lines = content.split("\n")
            if lines[-1].strip() == "```":
                content = "\n".join(lines[1:-1])
            else:
                content = "\n".join(lines[1:])
        elif content.startswith("```json"):
            lines = content.split("\n")
            if lines[-1].strip() == "```":
                content = "\n".join(lines[1:-1])
            else:
                content = "\n".join(lines[1:])

        detected = json.loads(content)
        # Filtrar solo nombres que estÃ¡n en available_people
        detected = [name for name in detected if name in available_people]
        return detected
    except Exception as e:
        st.sidebar.warning(f"Error en detecciÃ³n de personas: {e}. Usando agente por defecto.")
        return []


def combine_contexts(contexts: Dict[str, Tuple[str, List[Document]]]) -> str:
    """
    Combina contextos de mÃºltiples agentes en un formato estructurado.
    
    Args:
        contexts: Dict con nombre_persona -> (contexto_texto, documentos)
        
    Returns:
        String con contexto combinado y organizado por persona
    """
    combined = []
    for person_name, (context_text, _) in contexts.items():
        combined.append(f"=== CV de {person_name} ===\n{context_text}\n")
    return "\n".join(combined)


def route_query(
    query: str,
    agents: Dict[str, PersonAgent],
    default_agent_name: str,
    detection_llm: ChatOpenAI,
    qa_llm: ChatOpenAI,
    prompt_template: PromptTemplate,
) -> Tuple[str, Dict[str, List[Document]], List[str]]:
    """
    Enruta una query a los agentes apropiados y genera respuesta.
    
    Args:
        query: Pregunta del usuario
        agents: Dict con nombre -> PersonAgent
        default_agent_name: Nombre del agente por defecto (alumno)
        detection_llm: Modelo LLM para detecciÃ³n de personas
        qa_llm: Modelo LLM para generar respuestas
        prompt_template: Template del prompt
        
    Returns:
        Tupla con (respuesta, dict de docs por agente, lista de agentes usados)
    """
    available_people = list(agents.keys())
    
    # Detectar si es una pregunta comparativa que requiere todos los CVs
    is_comparative = is_comparative_query(query, detection_llm)
    
    # Si es comparativa, usar todos los agentes disponibles
    if is_comparative and len(agents) > 1:
        contexts_dict = {}
        docs_dict = {}
        for person_name, agent in agents.items():
            context, docs = agent.retrieve(query)
            contexts_dict[person_name] = (context, docs)
            docs_dict[person_name] = docs
        
        combined_context = combine_contexts(contexts_dict)
        formatted_prompt = prompt_template.format(
            context=combined_context, question=query
        )
        response = qa_llm.invoke(formatted_prompt)
        return response.content, docs_dict, list(agents.keys())
    
    # DetecciÃ³n normal de personas mencionadas
    detected_people = detect_people_in_query(query, available_people, detection_llm)

    # Si no se detecta ninguna persona, usar agente por defecto
    if not detected_people:
        if default_agent_name in agents:
            agent = agents[default_agent_name]
            context, docs = agent.retrieve(query)
            formatted_prompt = prompt_template.format(
                context=f"=== CV de {default_agent_name} ===\n{context}",
                question=query,
            )
            response = qa_llm.invoke(formatted_prompt)
            return (
                response.content,
                {default_agent_name: docs},
                [default_agent_name],
            )
        else:
            return (
                "No hay agentes disponibles. Por favor carga al menos un CV.",
                {},
                [],
            )

    # Si se detecta una persona, usar su agente
    if len(detected_people) == 1:
        person_name = detected_people[0]
        agent = agents[person_name]
        context, docs = agent.retrieve(query)
        formatted_prompt = prompt_template.format(
            context=f"=== CV de {person_name} ===\n{context}",
            question=query,
        )
        response = qa_llm.invoke(formatted_prompt)
        return response.content, {person_name: docs}, [person_name]

    # Si se detectan mÃºltiples personas, combinar contextos
    contexts_dict = {}
    docs_dict = {}
    for person_name in detected_people:
        agent = agents[person_name]
        context, docs = agent.retrieve(query)
        contexts_dict[person_name] = (context, docs)
        docs_dict[person_name] = docs

    combined_context = combine_contexts(contexts_dict)
    formatted_prompt = prompt_template.format(
        context=combined_context, question=query
    )
    response = qa_llm.invoke(formatted_prompt)
    return response.content, docs_dict, detected_people


def create_prompt_template() -> PromptTemplate:
    """Crea el template del prompt para respuestas."""
    template = """Eres un asistente que responde preguntas sobre CVs de integrantes de un equipo.

Debes responder **Ãºnicamente** usando la informaciÃ³n del contexto proporcionado.
Si la respuesta no estÃ¡ en el contexto, responde exactamente:
"No tengo esa informaciÃ³n en el/los CV(s)."

Cuando el contexto incluye mÃºltiples CVs:
- Si la pregunta es comparativa (ej: "Â¿quiÃ©n es mejor para...?", "Â¿quiÃ©n tiene mÃ¡s...?"), compara explÃ­citamente entre todos los candidatos y proporciona una recomendaciÃ³n clara.
- Organiza tu respuesta claramente indicando de quiÃ©n es cada informaciÃ³n.
- Para preguntas comparativas, estructura tu respuesta comparando punto por punto y concluye con una recomendaciÃ³n.

ğŸ“„ CONTEXTO:
{context}

â“ PREGUNTA:
{question}

ğŸ§  RESPUESTA clara, en espaÃ±ol y bien estructurada:"""
    return PromptTemplate(input_variables=["context", "question"], template=template)


# -----------------------------
# Sidebar: configuraciÃ³n y carga de CVs
# -----------------------------
st.sidebar.header("âš™ï¸ ConfiguraciÃ³n")

# API Key
env_api_key = os.getenv("OPENAI_API_KEY") or ""
api_key = st.sidebar.text_input(
    "ğŸ”‘ OpenAI API Key",
    type="password",
    value=env_api_key,
    help="Tu clave de OpenAI. No se guarda en ningÃºn lado.",
)

if not api_key:
    st.sidebar.warning("IngresÃ¡ tu OpenAI API Key para continuar.")
    st.stop()

# Modelo y temperatura
model_name = st.sidebar.selectbox(
    "Modelo",
    options=["gpt-4o-mini", "gpt-4o", "gpt-3.5-turbo"],
    index=0,
)
temperature = st.sidebar.slider("Creatividad (temperature)", 0.0, 1.0, 0.2, 0.05)

# LLM para detecciÃ³n (usar modelo rÃ¡pido)
detection_llm = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0,
    api_key=api_key,
)

# LLM para respuestas
qa_llm = ChatOpenAI(
    model=model_name,
    temperature=temperature,
    api_key=api_key,
)

st.sidebar.markdown("---")
st.sidebar.markdown("### ğŸ“„ CVs del Equipo (mÃ¡ximo 3)")

# Inicializar estado de agentes
if "agents" not in st.session_state:
    st.session_state.agents = {}
if "agent_names" not in st.session_state:
    st.session_state.agent_names = []
if "default_agent_name" not in st.session_state:
    st.session_state.default_agent_name = None

# Carga de CVs
MAX_AGENTS = 3
uploaded_files = []
agent_names_input = []

for i in range(MAX_AGENTS):
    st.sidebar.markdown(f"#### Persona {i+1}")
    name = st.sidebar.text_input(
        f"Nombre de la persona {i+1}",
        key=f"name_{i}",
        placeholder="Ej: Lucas Argento",
        help="Nombre de la persona (usado para detecciÃ³n en queries)",
    )
    uploaded_file = st.sidebar.file_uploader(
        f"CV {i+1} (PDF)",
        type=["pdf"],
        key=f"cv_{i}",
        help=f"Sube el CV de {name if name else 'la persona'}",
    )

    if uploaded_file is not None and name:
        uploaded_files.append((name, uploaded_file))
        agent_names_input.append(name)

# Establecer agente por defecto (primera persona cargada = alumno)
if agent_names_input and st.session_state.default_agent_name is None:
    st.session_state.default_agent_name = agent_names_input[0]
    st.sidebar.info(f"âœ… Agente por defecto: **{agent_names_input[0]}**")

# BotÃ³n para actualizar agentes
if st.sidebar.button("ğŸ”„ Cargar/Actualizar Agentes"):
    if not uploaded_files:
        st.sidebar.error("Por favor carga al menos un CV con su nombre.")
    else:
        with st.spinner("Creando agentes..."):
            new_agents = {}
            new_agent_names = []

            for name, uploaded_file in uploaded_files:
                # Guardar PDF temporalmente con nombre Ãºnico
                # Resetear el puntero del archivo para asegurar que leemos desde el inicio
                uploaded_file.seek(0)
                with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf", prefix=f"cv_{name}_") as tmp:
                    tmp.write(uploaded_file.read())
                    tmp_path = tmp.name

                try:
                    # Cargar documentos del PDF
                    docs = load_docs_from_pdf(tmp_path)
                    
                    # Verificar que se cargaron documentos
                    if not docs:
                        st.sidebar.warning(f"âš ï¸ El CV de {name} estÃ¡ vacÃ­o o no se pudo leer.")
                        continue
                    
                    # Crear agente con documentos Ãºnicos
                    agent = PersonAgent(name, docs, model_name, temperature, api_key)
                    new_agents[name] = agent
                    new_agent_names.append(name)
                    st.sidebar.success(f"âœ… Agente creado para **{name}** ({len(docs)} pÃ¡ginas)")
                    
                    # Limpiar archivo temporal despuÃ©s de cargar
                    try:
                        os.unlink(tmp_path)
                    except Exception:
                        pass  # Ignorar errores al eliminar archivo temporal
                        
                except Exception as e:
                    st.sidebar.error(f"âŒ Error creando agente para {name}: {e}")
                    # Intentar limpiar archivo temporal en caso de error
                    try:
                        os.unlink(tmp_path)
                    except Exception:
                        pass

            if new_agents:
                st.session_state.agents = new_agents
                st.session_state.agent_names = new_agent_names
                if not st.session_state.default_agent_name:
                    st.session_state.default_agent_name = new_agent_names[0]
                st.sidebar.success(f"âœ… {len(new_agents)} agente(s) listo(s)")

# Mostrar agentes activos
st.sidebar.markdown("---")
st.sidebar.markdown("### ğŸ¤– Agentes Activos")
if st.session_state.agents:
    for name in st.session_state.agent_names:
        default_badge = " (Por defecto)" if name == st.session_state.default_agent_name else ""
        st.sidebar.markdown(f"- **{name}**{default_badge}")
else:
    st.sidebar.info("No hay agentes cargados. Sube CVs y haz clic en 'Cargar/Actualizar Agentes'.")

# -----------------------------
# Verificar que hay agentes cargados
# -----------------------------
if not st.session_state.agents:
    st.info(
        "ğŸ‘‹ **Bienvenido al Sistema Multi-Agente para CVs**\n\n"
        "1. En la barra lateral, ingresa el nombre de cada integrante del equipo\n"
        "2. Sube su CV en PDF\n"
        "3. Haz clic en 'Cargar/Actualizar Agentes'\n"
        "4. Â¡Empieza a hacer preguntas! El sistema detectarÃ¡ automÃ¡ticamente sobre quiÃ©n preguntas.\n\n"
        "**Ejemplos de preguntas:**\n"
        "- 'Â¿QuÃ© experiencia tiene [Nombre]?' (pregunta sobre una persona)\n"
        "- 'Compara las habilidades de [Nombre1] y [Nombre2]' (pregunta sobre mÃºltiples personas)\n"
        "- 'Â¿QuÃ© tecnologÃ­as usa?' (sin mencionar nombre, usa agente por defecto)"
    )
    st.stop()

# Crear prompt template
prompt_template = create_prompt_template()

# -----------------------------
# Historial de chat
# -----------------------------
if "history" not in st.session_state:
    st.session_state.history = []

# Mostrar historial previo
for msg in st.session_state.history:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# -----------------------------
# Input tipo chat
# -----------------------------
query = st.chat_input("Pregunta sobre los CVs del equipo...")

if query:
    # Mostrar mensaje del usuario
    with st.chat_message("user"):
        st.markdown(query)

    # Ejecutar routing y RAG
    with st.spinner("ğŸ¤” Procesando consulta..."):
        answer, docs_dict, active_agents = route_query(
            query,
            st.session_state.agents,
            st.session_state.default_agent_name,
            detection_llm,
            qa_llm,
            prompt_template,
        )

    # Mostrar agentes activos
    if active_agents:
        agent_badges = " | ".join([f"ğŸ¤– **{name}**" for name in active_agents])
        st.info(f"**Agentes activos:** {agent_badges}")

    # Mostrar respuesta del asistente
    with st.chat_message("assistant"):
        st.markdown(answer)

    # Guardar en historial
    st.session_state.history.append({"role": "user", "content": query})
    st.session_state.history.append({"role": "assistant", "content": answer})

    # Mostrar contexto recuperado por agente
    if docs_dict:
        with st.expander("ğŸ” Ver fragmentos de CVs usados para responder"):
            for person_name, docs in docs_dict.items():
                st.markdown(f"### ğŸ“„ CV de **{person_name}**")
                for i, d in enumerate(docs, start=1):
                    st.markdown(f"**Fragmento {i}:**")
                    st.write(
                        d.page_content[:600]
                        + ("..." if len(d.page_content) > 600 else "")
                    )
                    st.markdown("---")

# Mensaje inicial si todavÃ­a no hay conversaciÃ³n
if not st.session_state.history and query is None:
    st.info(
        "ğŸ’¡ **Ejemplos de preguntas:**\n\n"
        f"- 'Â¿QuÃ© experiencia tiene {st.session_state.agent_names[0] if st.session_state.agent_names else "[Nombre]"}?'\n"
        "- 'Â¿DÃ³nde estudia [Nombre]?'\n"
        "- 'Compara las habilidades tÃ©cnicas de [Nombre1] y [Nombre2]'\n"
        "- 'Â¿QuÃ© tecnologÃ­as usa?' (sin mencionar nombre, usa agente por defecto)"
    )

